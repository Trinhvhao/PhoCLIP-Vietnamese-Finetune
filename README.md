<div align="center">

# PhoCLIP - Vietnamese Image-Text Matching

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

**A Vietnamese CLIP model for image-text retrieval using PhoBERT and ResNet50**

[Features](#features) â€¢ [Installation](#installation) â€¢ [Usage](#usage) â€¢ [Model Architecture](#model-architecture) â€¢ [Training](#training) â€¢ [Results](#results)

</div>

---

## ğŸ“‹ Overview

PhoCLIP is a Vietnamese adaptation of the CLIP (Contrastive Language-Image Pre-training) model, designed specifically for Vietnamese image-text matching tasks. It combines:

- **Text Encoder**: PhoBERT-large (Vietnamese BERT)
- **Image Encoder**: ResNet50 (pretrained on ImageNet)
- **Projection Heads**: Maps both modalities to a shared embedding space

## âœ¨ Features

- ğŸ‡»ğŸ‡³ **Vietnamese Language Support** - Optimized for Vietnamese text with PhoBERT
- ğŸ–¼ï¸ **Multi-Dataset Training** - Trained on COCO, Flickr, KTVIC, and OpenViIC datasets
- ğŸ” **Bidirectional Search** - Find images from text or text from images
- âš¡ **Fast Inference** - Optimized for GPU acceleration
- ğŸ“Š **High Accuracy** - Achieves competitive Top-5 accuracy on Vietnamese datasets

## ğŸš€ Installation

### Prerequisites

```bash
# Python 3.8 or higher
python --version

# CUDA-enabled GPU (recommended)
nvidia-smi
```

### Install Dependencies

```bash
pip install torch torchvision torchaudio
pip install timm transformers
pip install py_vncorenlp
pip install pandas numpy opencv-python albumentations
pip install matplotlib seaborn tqdm
pip install jsonlines
```

### Download VnCoreNLP

```python
import py_vncorenlp
py_vncorenlp.download_model()
```

## ğŸ“– Usage

### Quick Start

```python
import torch
from phoclip import CLIPModel, find_matches
from transformers import AutoTokenizer

# Load model
model = CLIPModel()
model.load_state_dict(torch.load("best.pt"))
model.eval()

# Search images by text
find_matches(
    model,
    image_embeddings,
    image_filenames,
    text="xe hÆ¡i Ä‘áº­u trÆ°á»›c ngÃ´i nhÃ ",
    n=25
)
```

### Training from Scratch

```python
from phoclip import main

# Configure training parameters in CFG class
# Then run training
main()
```

### Extract Embeddings

```python
from phoclip import get_image_embeddings, get_text_embeddings

# Get image embeddings
model, img_embeds = get_image_embeddings(valid_df, "best.pt")

# Get text embeddings
model, txt_embeds = get_text_embeddings(valid_df, "best.pt")
```

## ğŸ—ï¸ Model Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PhoCLIP Model                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚ Image Input  â”‚         â”‚  Text Input  â”‚         â”‚
â”‚   â”‚  (224x224)   â”‚         â”‚  (max_len)   â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚          â”‚                        â”‚                 â”‚
â”‚          â–¼                        â–¼                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚  ResNet50    â”‚         â”‚PhoBERT-large â”‚         â”‚
â”‚   â”‚  (2048-dim)  â”‚         â”‚  (1024-dim)  â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚          â”‚                        â”‚                 â”‚
â”‚          â–¼                        â–¼                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚ Projection   â”‚         â”‚ Projection   â”‚         â”‚
â”‚   â”‚    Head      â”‚         â”‚    Head      â”‚         â”‚
â”‚   â”‚  (512-dim)   â”‚         â”‚  (512-dim)   â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚          â”‚                        â”‚                 â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                     â–¼                               â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚          â”‚  Contrastive Loss    â”‚                   â”‚
â”‚          â”‚  (Temperature=1.0)   â”‚                   â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Training

### Configuration

Key hyperparameters in `CFG` class:

```python
batch_size = 64
epochs = 1
image_encoder_lr = 1e-4
text_encoder_lr = 1e-5
head_lr = 1e-3
temperature = 1.0
projection_dim = 512
max_length = 70
```

### Datasets

The model is trained on multiple Vietnamese image-caption datasets:

| Dataset | Images | Captions | Language |
|---------|--------|----------|----------|
| COCO (translated) | 123,287 | 616,767 | Vietnamese |
| Flickr30k | 31,783 | 158,915 | Vietnamese |
| KTVIC | 4,327 | 21,635 | Vietnamese |
| OpenViIC | 9,328 | 46,640 | Vietnamese |

### Training Pipeline

1. **Data Preprocessing**
   - Image resizing to 224x224
   - Vietnamese text segmentation with VnCoreNLP
   - Normalization

2. **Model Training**
   - Contrastive learning with symmetric cross-entropy loss
   - AdamW optimizer with learning rate scheduling
   - Early stopping based on validation loss

3. **Evaluation**
   - Top-K accuracy (K=1, 5, 10)
   - Image-to-text and text-to-image retrieval

## ğŸ“Š Results

### Top-K Accuracy

| Metric | Score |
|--------|-------|
| Top-1 | TBD |
| Top-5 | TBD |
| Top-10 | TBD |

### Example Queries

**Text Query**: "xe hÆ¡i Ä‘áº­u trÆ°á»›c ngÃ´i nhÃ "
- Successfully retrieves images of cars parked in front of houses

**Image Query**: Upload an image
- Returns similar images from the database

## ğŸ”§ Advanced Usage

### Custom Dataset

```python
import pandas as pd

# Prepare your data
df = pd.DataFrame({
    'image': ['img1.jpg', 'img2.jpg'],
    'caption': ['mÃ´ táº£ 1', 'mÃ´ táº£ 2']
})

# Build dataloader
train_loader = build_loaders(df, tokenizer, mode="train")
```

### Fine-tuning

```python
# Freeze image encoder
CFG.image_encoder_trainable = False

# Train only text encoder and projection heads
model = CLIPModel()
# ... training code
```

## ğŸ“ Project Structure

```
Phoclip/
â”œâ”€â”€ phoclip.py          # Main implementation
â”œâ”€â”€ phoclip.ipynb       # Jupyter notebook
â”œâ”€â”€ README.md           # This file
â”œâ”€â”€ best.pt             # Trained model weights (after training)
â””â”€â”€ images/             # Image dataset directory
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- **PhoBERT**: [VinAI Research](https://github.com/VinAIResearch/PhoBERT)
- **CLIP**: [OpenAI](https://github.com/openai/CLIP)
- **VnCoreNLP**: [Vietnamese NLP Toolkit](https://github.com/vncorenlp/VnCoreNLP)
- **Original PhoCLIP**: [ducngg/PhoCLIP](https://github.com/ducngg/PhoCLIP)
- **Datasets**: COCO, Flickr30k, KTVIC, OpenViIC

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub.

---

<div align="center">

Made with â¤ï¸ for Vietnamese NLP

</div>
