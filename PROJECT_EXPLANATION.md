# ğŸ¯ Giáº£i ThÃ­ch Chi Tiáº¿t Dá»± Ãn PhoCLIP

## ğŸ“– Giá»›i Thiá»‡u BÃ i ToÃ¡n

### Image-Text Matching (GhÃ©p Cáº·p áº¢nh-VÄƒn Báº£n)

**Má»¥c tiÃªu**: XÃ¢y dá»±ng mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng:
- TÃ¬m áº£nh phÃ¹ há»£p tá»« mÃ´ táº£ vÄƒn báº£n (Text â†’ Image)
- TÃ¬m mÃ´ táº£ vÄƒn báº£n phÃ¹ há»£p tá»« áº£nh (Image â†’ Text)
- Äo Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a áº£nh vÃ  vÄƒn báº£n

**VÃ­ dá»¥:**
```
Input: "Má»™t chiáº¿c xe hÆ¡i mÃ u Ä‘á» Ä‘áº­u trÆ°á»›c ngÃ´i nhÃ "
Output: [áº¢nh xe hÆ¡i Ä‘á» trÆ°á»›c nhÃ ]
```

### ThÃ¡ch Thá»©c
- áº¢nh vÃ  vÄƒn báº£n á»Ÿ 2 khÃ´ng gian khÃ¡c nhau
- Ãt mÃ´ hÃ¬nh há»— trá»£ tá»‘t tiáº¿ng Viá»‡t
- Cáº§n xá»­ lÃ½ hÃ ng trÄƒm nghÃ¬n cáº·p áº£nh-vÄƒn báº£n

---

## ğŸ’¡ Ã TÆ°á»Ÿng PhoCLIP

### Giáº£i PhÃ¡p
Káº¿t há»£p:
- **PhoBERT**: Hiá»ƒu tiáº¿ng Viá»‡t
- **ResNet50**: TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng áº£nh
- **Contrastive Learning**: Há»c khÃ´ng gian chung

### á»¨ng Dá»¥ng
- TÃ¬m kiáº¿m áº£nh báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn
- Chatbot mÃ´ táº£ áº£nh
- Há»— trá»£ ngÆ°á»i khiáº¿m thá»‹
- E-commerce: TÃ¬m sáº£n pháº©m báº±ng mÃ´ táº£

---

## ğŸ—ï¸ Kiáº¿n TrÃºc MÃ´ HÃ¬nh


### Luá»“ng Dá»¯ Liá»‡u

```
Image (224x224) â”€â”€â†’ ResNet50 â”€â”€â†’ [2048-dim] â”€â”€â†’ Projection â”€â”€â†’ [512-dim]
                                                      â†“
                                              Cosine Similarity
                                                      â†“
Text (Vietnamese) â”€â”€â†’ PhoBERT â”€â”€â†’ [1024-dim] â”€â”€â†’ Projection â”€â”€â†’ [512-dim]
```

### CÃ¡c ThÃ nh Pháº§n

**1. Image Encoder (ResNet50)**
- Input: áº¢nh RGB 224x224
- Output: Vector 2048 chiá»u
- Pretrained trÃªn ImageNet

**2. Text Encoder (PhoBERT-large)**
- Input: CÃ¢u tiáº¿ng Viá»‡t (max 70 tokens)
- Output: Vector 1024 chiá»u
- Pretrained trÃªn corpus tiáº¿ng Viá»‡t

**3. Projection Heads**
- Chiáº¿u cáº£ 2 encoders vá» khÃ´ng gian 512 chiá»u
- Sá»­ dá»¥ng GELU activation + LayerNorm

**4. Contrastive Loss**
- Tá»‘i Ä‘a hÃ³a Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cá»§a cáº·p (áº£nh, caption) Ä‘Ãºng
- Tá»‘i thiá»ƒu hÃ³a Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cá»§a cÃ¡c cáº·p sai

---

## ğŸ”„ Luá»“ng Code Chi Tiáº¿t

### 1. Chuáº©n Bá»‹ Dá»¯ Liá»‡u


**File: phoclip.py (dÃ²ng 1-100)**

```python
# BÆ°á»›c 1: Load VnCoreNLP Ä‘á»ƒ phÃ¢n Ä‘oáº¡n tá»« tiáº¿ng Viá»‡t
rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=["wseg"])

# BÆ°á»›c 2: Äá»‹nh nghÄ©a config
class CFG:
    batch_size = 64
    image_path = "images/"
    text_encoder_model = "vinai/phobert-large"
    image_encoder_model = "resnet50"
```

**Táº¡i sao cáº§n phÃ¢n Ä‘oáº¡n tá»«?**
- Tiáº¿ng Viá»‡t: "lÃ m viá»‡c" â†’ "lÃ m_viá»‡c" (1 token)
- PhoBERT yÃªu cáº§u input Ä‘Ã£ Ä‘Æ°á»£c phÃ¢n Ä‘oáº¡n

### 2. Táº£i vÃ  Xá»­ LÃ½ Datasets

**File: phoclip.py (dÃ²ng 100-300)**

```python
# Táº£i COCO
df_coco = load_jsonl("cocopath_train.jsonl")

# Táº£i Flickr
df_flickr = pd.DataFrame({'image': [...], 'caption': [...]})

# Gá»™p táº¥t cáº£
data_df = pd.concat([df_coco, df_flickr, df_ktvic, df_openviic])
```

**Xá»­ lÃ½ áº£nh:**
- Copy áº£nh tá»« nhiá»u nguá»“n vÃ o 1 folder `images/`
- Äá»•i tÃªn vá»›i prefix: `COCO_`, `flickr-`, `ktvic-`, `openviic-`

### 3. Dataset vÃ  DataLoader

**File: phoclip.py (dÃ²ng 300-400)**


```python
class CLIPDataset:
    def __getitem__(self, idx):
        # 1. Äá»c áº£nh
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # 2. Transform áº£nh (resize, normalize)
        image = self.transforms(image=image)['image']
        
        # 3. PhÃ¢n Ä‘oáº¡n tá»« tiáº¿ng Viá»‡t
        caption = CFG.segmenter(caption)  # "lÃ m viá»‡c" â†’ "lÃ m_viá»‡c"
        
        # 4. Tokenize text
        encoded = tokenizer(caption, max_length=70)
        
        return {'image': image, 'input_ids': ..., 'attention_mask': ...}
```

**Augmentation:**
- Resize vá» 224x224
- Normalize theo ImageNet mean/std

### 4. Äá»‹nh NghÄ©a MÃ´ HÃ¬nh

**File: phoclip.py (dÃ²ng 400-500)**

```python
class CLIPModel(nn.Module):
    def __init__(self):
        self.image_encoder = ImageEncoder()      # ResNet50
        self.text_encoder = TextEncoder()        # PhoBERT
        self.image_projection = ProjectionHead() # 2048â†’512
        self.text_projection = ProjectionHead()  # 1024â†’512
    
    def forward(self, batch):
        # Encode
        img_features = self.image_encoder(batch["image"])
        txt_features = self.text_encoder(batch["input_ids"])
        
        # Project
        img_emb = self.image_projection(img_features)
        txt_emb = self.text_projection(txt_features)
        
        # Compute loss
        return contrastive_loss(img_emb, txt_emb)
```

### 5. Contrastive Loss


**CÃ¡ch hoáº¡t Ä‘á»™ng:**

```python
# Batch size = 4
images = [img1, img2, img3, img4]
texts = [txt1, txt2, txt3, txt4]

# TÃ­nh similarity matrix (4x4)
similarity = text_emb @ image_emb.T

# Targets: diagonal = 1, others = 0
# txt1 â†” img1 = 1.0 (Ä‘Ãºng)
# txt1 â†” img2 = 0.0 (sai)
# txt1 â†” img3 = 0.0 (sai)
# txt1 â†” img4 = 0.0 (sai)

# Loss: Cross-entropy
loss = -log(softmax(similarity))
```

**Má»¥c tiÃªu:**
- Cáº·p Ä‘Ãºng (txt1, img1): similarity cao
- Cáº·p sai (txt1, img2): similarity tháº¥p

### 6. Training Loop

**File: phoclip.py (dÃ²ng 500-600)**

```python
def train_epoch(model, train_loader, optimizer):
    for batch in train_loader:
        # Forward
        loss = model(batch)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

**Optimizer:**
- AdamW vá»›i learning rates khÃ¡c nhau:
  - Image encoder: 1e-4
  - Text encoder: 1e-5
  - Projection heads: 1e-3

### 7. Inference

**File: phoclip.py (dÃ²ng 600-700)**


```python
def find_matches(model, query_text, database_embeddings, n=25):
    # 1. Encode query text
    text_emb = model.text_encoder(query_text)
    text_emb = model.text_projection(text_emb)
    
    # 2. Normalize embeddings
    text_emb = F.normalize(text_emb, p=2, dim=-1)
    db_emb = F.normalize(database_embeddings, p=2, dim=-1)
    
    # 3. Compute similarity
    similarity = text_emb @ db_emb.T
    
    # 4. Get top-k
    values, indices = torch.topk(similarity, n)
    
    return [image_filenames[idx] for idx in indices]
```

**VÃ­ dá»¥ sá»­ dá»¥ng:**
```python
# TÃ¬m áº£nh tá»« text
matches = find_matches(
    model, 
    text="xe hÆ¡i Ä‘áº­u trÆ°á»›c ngÃ´i nhÃ ",
    database_embeddings=all_image_embeddings,
    n=25
)
```

---

## ğŸ“Š CÃ¡ch Hoáº¡t Äá»™ng (Step by Step)

### Training Phase

**BÆ°á»›c 1: Load Data**
```
COCO: 123,287 áº£nh + 616,767 captions
Flickr: 31,783 áº£nh + 158,915 captions
â†’ Total: 168,725 áº£nh + 843,957 captions
```

**BÆ°á»›c 2: Preprocessing**
```
Image: Resize(224,224) â†’ Normalize
Text: PhÃ¢n Ä‘oáº¡n tá»« â†’ Tokenize (max_len=70)
```

**BÆ°á»›c 3: Forward Pass**
```
Batch (64 samples):
  Images [64, 3, 224, 224] â†’ ResNet50 â†’ [64, 2048] â†’ Proj â†’ [64, 512]
  Texts [64, 70] â†’ PhoBERT â†’ [64, 1024] â†’ Proj â†’ [64, 512]
```

**BÆ°á»›c 4: Compute Loss**
```
Similarity Matrix [64, 64]
Target: Identity matrix (diagonal = 1)
Loss: Cross-entropy
```

**BÆ°á»›c 5: Backward & Update**
```
loss.backward()
optimizer.step()
```

### Inference Phase


**BÆ°á»›c 1: Extract All Image Embeddings**
```python
# Cháº¡y 1 láº§n duy nháº¥t
for batch in dataloader:
    img_emb = model.image_encoder(batch["image"])
    img_emb = model.image_projection(img_emb)
    all_embeddings.append(img_emb)

# LÆ°u vÃ o file
torch.save(all_embeddings, "image_embeddings.pt")
```

**BÆ°á»›c 2: Search**
```python
# Query
query = "xe hÆ¡i mÃ u Ä‘á»"
query_emb = encode_text(query)  # [1, 512]

# Compute similarity vá»›i táº¥t cáº£ áº£nh
similarity = query_emb @ all_embeddings.T  # [1, 168725]

# Top-25
top_indices = similarity.topk(25)
results = [images[idx] for idx in top_indices]
```

**Tá»‘c Ä‘á»™:**
- Extract embeddings: ~10 phÃºt (1 láº§n duy nháº¥t)
- Search: <0.1 giÃ¢y (real-time)

---

## ğŸ“ Kiáº¿n Thá»©c Cáº§n Thiáº¿t

### 1. Deep Learning CÆ¡ Báº£n
- Neural Networks
- Backpropagation
- Optimization (SGD, Adam)

### 2. Computer Vision
- CNN (Convolutional Neural Networks)
- ResNet architecture
- Image preprocessing

### 3. NLP
- Transformer architecture
- BERT model
- Tokenization

### 4. PyTorch
- nn.Module
- DataLoader
- Autograd

---

## ğŸš€ á»¨ng Dá»¥ng Thá»±c Táº¿

### 1. TÃ¬m Kiáº¿m áº¢nh Google-style
```python
query = "con mÃ¨o Ä‘ang ngá»§"
results = search_images(query)
# â†’ Tráº£ vá» 100 áº£nh mÃ¨o ngá»§
```

### 2. E-commerce
```python
query = "Ã¡o sÆ¡ mi tráº¯ng tay dÃ i"
products = search_products(query)
# â†’ Hiá»ƒn thá»‹ sáº£n pháº©m phÃ¹ há»£p
```

### 3. Há»— Trá»£ NgÆ°á»i Khiáº¿m Thá»‹
```python
image = capture_camera()
description = describe_image(image)
text_to_speech(description)
# â†’ "Má»™t chiáº¿c xe hÆ¡i Ä‘ang Ä‘áº­u bÃªn Ä‘Æ°á»ng"
```

### 4. Tá»• Chá»©c ThÆ° Viá»‡n áº¢nh
```python
# Tá»± Ä‘á»™ng gáº¯n tag
for image in photo_library:
    tags = generate_tags(image)
    # â†’ ["xe hÆ¡i", "Ä‘Æ°á»ng phá»‘", "ban ngÃ y"]
```

---

## ğŸ“ˆ Káº¿t Quáº£ Mong Äá»£i

### Metrics
- **Top-1 Accuracy**: ~40-50%
- **Top-5 Accuracy**: ~70-80%
- **Top-10 Accuracy**: ~85-90%

### So SÃ¡nh
| Model | Top-5 Accuracy |
|-------|----------------|
| Random | 0.003% |
| PhoCLIP | ~75% |
| CLIP (English) | ~85% |

---

## ğŸ” Debug vÃ  Tá»‘i Æ¯u

### Kiá»ƒm Tra Model
```python
# Test forward pass
batch = next(iter(train_loader))
loss = model(batch)
print(f"Loss: {loss.item()}")  # Should decrease over time
```

### Visualize Embeddings
```python
from sklearn.manifold import TSNE

# Reduce to 2D
embeddings_2d = TSNE(n_components=2).fit_transform(embeddings)

# Plot
plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])
```

### Monitor Training
```python
# Track metrics
wandb.log({
    "loss": loss.item(),
    "lr": get_lr(optimizer),
    "epoch": epoch
})
```

---

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o

- [CLIP Paper](https://arxiv.org/abs/2103.00020)
- [PhoBERT](https://github.com/VinAIResearch/PhoBERT)
- [ResNet Paper](https://arxiv.org/abs/1512.03385)
- [Contrastive Learning](https://arxiv.org/abs/2002.05709)
