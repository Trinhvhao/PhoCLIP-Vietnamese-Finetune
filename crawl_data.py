#!/usr/bin/env python3
"""
Script t·ª± ƒë·ªông t·∫£i v√† chu·∫©n b·ªã d·ªØ li·ªáu cho PhoCLIP
H·ªó tr·ª£: COCO, Flickr30k, KTVIC, OpenViIC
"""

import os
import sys
import json
import argparse
import subprocess
from pathlib import Path
from tqdm import tqdm
import requests
import zipfile
import shutil

class DataCrawler:
    def __init__(self, base_dir="data"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        
    def download_file(self, url, dest_path, desc="Downloading"):
        """T·∫£i file v·ªõi progress bar"""
        response = requests.get(url, stream=True)
        total_size = int(response.headers.get('content-length', 0))
        
        with open(dest_path, 'wb') as f, tqdm(
            desc=desc,
            total=total_size,
            unit='iB',
            unit_scale=True,
            unit_divisor=1024,
        ) as pbar:
            for data in response.iter_content(chunk_size=1024):
                size = f.write(data)
                pbar.update(size)
    
    def extract_zip(self, zip_path, extract_to):
        """Gi·∫£i n√©n file zip"""
        print(f"üì¶ Extracting {zip_path}...")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_to)
        print("‚úÖ Extraction complete")
    
    def download_coco(self):
        """T·∫£i COCO dataset"""
        print("\n" + "="*60)
        print("üì• DOWNLOADING COCO DATASET")
        print("="*60)
        
        coco_dir = self.base_dir / "coco"
        coco_dir.mkdir(exist_ok=True)
        
        # URLs
        urls = {
            "train2014": "http://images.cocodataset.org/zips/train2014.zip",
            "val2014": "http://images.cocodataset.org/zips/val2014.zip",
            "annotations": "http://images.cocodataset.org/annotations/annotations_trainval2014.zip"
        }
        
        for name, url in urls.items():
            zip_path = coco_dir / f"{name}.zip"
            
            if zip_path.exists():
                print(f"‚è≠Ô∏è  {name}.zip already exists, skipping download")
            else:
                print(f"\nüì• Downloading {name}...")
                self.download_file(url, zip_path, desc=name)
            
            # Extract
            if not (coco_dir / name).exists():
                self.extract_zip(zip_path, coco_dir)
                # Clean up zip
                zip_path.unlink()
        
        print("\n‚úÖ COCO dataset downloaded successfully!")
        print(f"üìÇ Location: {coco_dir.absolute()}")

    
    def download_flickr(self):
        """H∆∞·ªõng d·∫´n t·∫£i Flickr30k (c·∫ßn Kaggle)"""
        print("\n" + "="*60)
        print("üì• FLICKR30K DATASET")
        print("="*60)
        
        flickr_dir = self.base_dir / "flickr"
        flickr_dir.mkdir(exist_ok=True)
        
        print("\n‚ö†Ô∏è  Flickr30k requires Kaggle account")
        print("\nüìù Steps to download:")
        print("1. Go to: https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset")
        print("2. Click 'Download' button")
        print("3. Extract to:", flickr_dir.absolute())
        print("\nOr use Kaggle API:")
        print(f"  kaggle datasets download -d hsankesara/flickr-image-dataset -p {flickr_dir}")
        print(f"  unzip {flickr_dir}/flickr-image-dataset.zip -d {flickr_dir}")
        
        # Check if already downloaded
        if (flickr_dir / "flickr30k_images").exists():
            print("\n‚úÖ Flickr30k images found!")
        else:
            print("\n‚ùå Flickr30k images not found. Please download manually.")
    
    def download_ktvic(self):
        """T·∫£i KTVIC dataset"""
        print("\n" + "="*60)
        print("üì• DOWNLOADING KTVIC DATASET")
        print("="*60)
        
        ktvic_dir = self.base_dir / "ktvic"
        
        if ktvic_dir.exists():
            print("‚è≠Ô∏è  KTVIC already exists, skipping...")
            return
        
        print("\nüì• Cloning KTVIC repository...")
        try:
            subprocess.run([
                "git", "clone",
                "https://github.com/uitnlp/KTVIC.git",
                str(ktvic_dir)
            ], check=True)
            print("‚úÖ KTVIC downloaded successfully!")
            print(f"üìÇ Location: {ktvic_dir.absolute()}")
        except subprocess.CalledProcessError:
            print("‚ùå Failed to clone KTVIC. Please install git or download manually.")
            print("   URL: https://github.com/uitnlp/KTVIC")
    
    def download_openviic(self):
        """T·∫£i OpenViIC dataset"""
        print("\n" + "="*60)
        print("üì• DOWNLOADING OPENVIIC DATASET")
        print("="*60)
        
        openviic_dir = self.base_dir / "openviic"
        
        if openviic_dir.exists():
            print("‚è≠Ô∏è  OpenViIC already exists, skipping...")
            return
        
        print("\nüì• Cloning OpenViIC repository...")
        try:
            subprocess.run([
                "git", "clone",
                "https://github.com/uitnlp/OpenViIC.git",
                str(openviic_dir)
            ], check=True)
            print("‚úÖ OpenViIC downloaded successfully!")
            print(f"üìÇ Location: {openviic_dir.absolute()}")
        except subprocess.CalledProcessError:
            print("‚ùå Failed to clone OpenViIC. Please install git or download manually.")
            print("   URL: https://github.com/uitnlp/OpenViIC")
    
    def verify_data(self):
        """Ki·ªÉm tra t√≠nh to√†n v·∫πn c·ªßa d·ªØ li·ªáu"""
        print("\n" + "="*60)
        print("üîç VERIFYING DATA")
        print("="*60)
        
        datasets = {
            "COCO train2014": self.base_dir / "coco" / "train2014",
            "COCO val2014": self.base_dir / "coco" / "val2014",
            "Flickr30k": self.base_dir / "flickr" / "flickr30k_images",
            "KTVIC": self.base_dir / "ktvic" / "train-images",
            "OpenViIC": self.base_dir / "openviic" / "images"
        }
        
        results = []
        for name, path in datasets.items():
            if path.exists():
                count = len(list(path.glob("*.jpg"))) + len(list(path.glob("*.png")))
                results.append((name, "‚úÖ", count))
            else:
                results.append((name, "‚ùå", 0))
        
        print("\nüìä Dataset Status:")
        print("-" * 60)
        for name, status, count in results:
            print(f"{status} {name:20s} - {count:,} images")
        print("-" * 60)
        
        total = sum(r[2] for r in results)
        print(f"\nüìà Total images: {total:,}")
    
    def download_all(self):
        """T·∫£i t·∫•t c·∫£ datasets"""
        print("\n" + "="*60)
        print("üöÄ DOWNLOADING ALL DATASETS")
        print("="*60)
        
        self.download_coco()
        self.download_flickr()
        self.download_ktvic()
        self.download_openviic()
        self.verify_data()
        
        print("\n" + "="*60)
        print("‚úÖ ALL DOWNLOADS COMPLETE!")
        print("="*60)
        print("\nüìù Next steps:")
        print("1. Run: python prepare_data.py")
        print("2. Run: python phoclip.py")


def main():
    parser = argparse.ArgumentParser(
        description="Download datasets for PhoCLIP training"
    )
    parser.add_argument(
        "--dataset",
        choices=["coco", "flickr", "ktvic", "openviic"],
        help="Download specific dataset"
    )
    parser.add_argument(
        "--all",
        action="store_true",
        help="Download all datasets"
    )
    parser.add_argument(
        "--verify",
        action="store_true",
        help="Verify downloaded data"
    )
    parser.add_argument(
        "--base-dir",
        default="data",
        help="Base directory for datasets (default: data)"
    )
    
    args = parser.parse_args()
    
    crawler = DataCrawler(base_dir=args.base_dir)
    
    if args.verify:
        crawler.verify_data()
    elif args.all:
        crawler.download_all()
    elif args.dataset:
        if args.dataset == "coco":
            crawler.download_coco()
        elif args.dataset == "flickr":
            crawler.download_flickr()
        elif args.dataset == "ktvic":
            crawler.download_ktvic()
        elif args.dataset == "openviic":
            crawler.download_openviic()
    else:
        parser.print_help()
        print("\nüí° Examples:")
        print("  python crawl_data.py --all")
        print("  python crawl_data.py --dataset coco")
        print("  python crawl_data.py --verify")


if __name__ == "__main__":
    main()
